{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSAI-LSTM Subtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from six.moves import range\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 80000\n",
    "DIGITS = 3\n",
    "REVERSE = False\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "chars = '0123456789-+'\n",
    "#RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "['560-026', '946+013', '009+004', '075-007', '820-008', '938-572', '676+007', '158+084', '151-067']\n",
      "['0534', '0959', '0013', '0068', '0812', '0366', '0683', '0242', '0084']\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "label = []\n",
    "seen = set()\n",
    "\n",
    "print('Generating data...')\n",
    "while len(data) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    \n",
    "    if(a<b):\n",
    "        a,b = b,a\n",
    "    operator = np.random.choice(list('+-'))    \n",
    "#     q = '{}-{}'.format(a, b)\n",
    "    q = str(a).zfill(3) + operator + str(b).zfill(DIGITS)    \n",
    "\n",
    "    if(q not in seen):\n",
    "        query = q + ' ' * (MAXLEN - len(q))\n",
    "        seen.add(query)\n",
    "        data.append(query)\n",
    "        \n",
    "        if(operator == \"+\"):\n",
    "            ans = str(a+b).zfill(DIGITS+1)\n",
    "        else:\n",
    "            ans = str(a-b).zfill(DIGITS+1)\n",
    "        \n",
    "        #ans += ' '* (DIGITS + 1 - len(ans))\n",
    "        \n",
    "        label.append(ans)\n",
    "        \n",
    "    \n",
    "print(data[:9])\n",
    "print(label[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    def __init__(self, chars):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "    \n",
    "    def encode(self, C, num_rows):\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return \"\".join(self.indices_char[i] for i in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "x.shape (80000, 7, 12)\n",
      "x[0] [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "ctable.decode(x[0]): 560-026\n",
      "y.shape (80000, 4, 12)\n",
      "y[0] [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "ctable.decode(y[0]): 0534\n"
     ]
    }
   ],
   "source": [
    "ctable = CharacterTable(chars)\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(data), MAXLEN, len(chars)))\n",
    "for i, sentence in enumerate(data):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "\n",
    "y = np.zeros((len(label), DIGITS + 1, len(chars)))\n",
    "for i, sentence in enumerate(label):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "print(\"x.shape\", x.shape)\n",
    "print(\"x[0]\", x[0])\n",
    "print(\"ctable.decode(x[0]):\", ctable.decode(x[0]))\n",
    "\n",
    "print(\"y.shape\", y.shape)\n",
    "print(\"y[0]\", y[0])\n",
    "print(\"ctable.decode(y[0]):\", ctable.decode(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "Training Data:\n",
      "(36000, 7, 12)\n",
      "(36000, 4, 12)\n",
      "Validation Data:\n",
      "(4000, 7, 12)\n",
      "(4000, 4, 12)\n",
      "Testing Data:\n",
      "(40000, 7, 12)\n",
      "(40000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# train_test_split\n",
    "train_x = x[:40000]\n",
    "train_y = y[:40000]\n",
    "\n",
    "test_x = x[40000:]\n",
    "test_y = y[40000:]\n",
    "\n",
    "split_at = len(train_x) - len(train_x) // 10\n",
    "print(len(train_x))\n",
    "\n",
    "(x_train, x_val) = train_x[:split_at], train_x[split_at:]\n",
    "(y_train, y_val) = train_y[:split_at], train_y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "print('Testing Data:')\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch data\n",
      "Training Data:\n",
      "torch.Size([36000, 7, 12])\n",
      "torch.Size([36000, 4, 12])\n",
      "Validation Data:\n",
      "torch.Size([4000, 7, 12])\n",
      "torch.Size([4000, 4, 12])\n",
      "Testing Data:\n",
      "torch.Size([40000, 7, 12])\n",
      "torch.Size([40000, 4, 12])\n"
     ]
    }
   ],
   "source": [
    "x_train_torch = torch.Tensor(x_train)\n",
    "y_train_torch = torch.Tensor(y_train)\n",
    "\n",
    "x_val_torch = torch.Tensor(x_val)\n",
    "y_val_torch = torch.Tensor(y_val)\n",
    "\n",
    "test_x_torch = torch.Tensor(test_x)\n",
    "test_y_torch = torch.Tensor(test_y)\n",
    "\n",
    "print('torch data')\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train_torch.size())\n",
    "print(y_train_torch.size())\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val_torch.size())\n",
    "print(y_val_torch.size())\n",
    "\n",
    "print('Testing Data:')\n",
    "print(test_x_torch.size())\n",
    "print(test_y_torch.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([128, 7, 12])\n",
      "torch.Size([128, 4, 12])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Data.TensorDataset(x_train_torch, y_train_torch)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    ")\n",
    "\n",
    "test_dataset = Data.TensorDataset(test_x_torch, test_y_torch)\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=test_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    ")\n",
    "\n",
    "for step, (batch_x, batch_y) in enumerate(train_loader):  # 每一步 loader 释放一小批数据用来学习\n",
    "    print(step)\n",
    "    print(batch_x.size())\n",
    "    print(batch_y.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pytorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "#USE_CUDA = False\n",
    "print(USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch.manual_seed(1)    # reproducible\n",
    "\n",
    "# Hyper Parameters\n",
    "EPOCH = 20               # train the training data n times, to save time, we just train 1 epoch\n",
    "BATCH_SIZE = 128\n",
    "HIDDEN_SIZE = HIDDEN_SIZE\n",
    "TIME_STEP = 7          # rnn time step / image height\n",
    "INPUT_SIZE = len(chars)         # rnn input size / image width\n",
    "LR = 0.01               # learning rate\n",
    "DOWNLOAD_MNIST = False   # set to True if haven't download the data\n",
    "\n",
    "# Data Loader for easy mini-batch return in training\n",
    "train_loader = train_loader\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=HIDDEN_SIZE,         # rnn hidden unit\n",
    "            num_layers=2,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(HIDDEN_SIZE*2 , len(chars))\n",
    "\n",
    "    def forward(self, x): \n",
    "        # x (batch, time_step, input_size)\n",
    "        # h_state (n_layers, batch, hidden_size)\n",
    "        # r_out (batch, time_step, output_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # h_state 也要作为 RNN 的一个输入，此為None\n",
    "        \n",
    "        outs = []    # 保存所有时间点的预测值\n",
    "        #for time_step in range(r_out.size(1)):    # 对每一个时间点计算 output\n",
    "        for time_step in range(-4, -1+1, 1):#只取最後4個time_step的輸出\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "        return  F.softmax(torch.stack(outs, dim=1), dim=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_CUDA: True\n",
      "RNN(\n",
      "  (rnn): LSTM(12, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (out): Linear(in_features=256, out_features=12, bias=True)\n",
      ")\n",
      "****************************** epoch: 0 ******************************\n",
      "loss: 0.0765417218208313\n",
      "accuracy: 0.0\n",
      "question: 550-003  pred: 4444 \u001b[91m☒\u001b[0m target: 0547\n",
      "question: 540-127  pred: 4444 \u001b[91m☒\u001b[0m target: 0413\n",
      "question: 563+052  pred: 4444 \u001b[91m☒\u001b[0m target: 0615\n",
      "question: 263-092  pred: 4444 \u001b[91m☒\u001b[0m target: 0171\n",
      "question: 848-252  pred: 4444 \u001b[91m☒\u001b[0m target: 0596\n",
      "question: 684+034  pred: 4444 \u001b[91m☒\u001b[0m target: 0718\n",
      "question: 643-039  pred: 4444 \u001b[91m☒\u001b[0m target: 0604\n",
      "question: 984+295  pred: 4444 \u001b[91m☒\u001b[0m target: 1279\n",
      "question: 410+014  pred: 4444 \u001b[91m☒\u001b[0m target: 0424\n",
      "question: 085-068  pred: 4444 \u001b[91m☒\u001b[0m target: 0017\n",
      "--------------------------------------------------\n",
      "loss: 0.0535803884267807\n",
      "accuracy: 0.0\n",
      "question: 319+000  pred: 0599 \u001b[91m☒\u001b[0m target: 0319\n",
      "question: 926+663  pred: 1997 \u001b[91m☒\u001b[0m target: 1589\n",
      "question: 372-085  pred: 0100 \u001b[91m☒\u001b[0m target: 0287\n",
      "question: 733+061  pred: 0347 \u001b[91m☒\u001b[0m target: 0794\n",
      "question: 292-052  pred: 0347 \u001b[91m☒\u001b[0m target: 0240\n",
      "question: 950-062  pred: 0947 \u001b[91m☒\u001b[0m target: 0888\n",
      "question: 055+035  pred: 0012 \u001b[91m☒\u001b[0m target: 0090\n",
      "question: 359-083  pred: 0100 \u001b[91m☒\u001b[0m target: 0276\n",
      "question: 460+028  pred: 0547 \u001b[91m☒\u001b[0m target: 0488\n",
      "question: 681-382  pred: 0000 \u001b[91m☒\u001b[0m target: 0299\n",
      "--------------------------------------------------\n",
      "loss: 0.04663388803601265\n",
      "accuracy: 0.0\n",
      "question: 807+008  pred: 0855 \u001b[91m☒\u001b[0m target: 0815\n",
      "question: 960-310  pred: 0550 \u001b[91m☒\u001b[0m target: 0650\n",
      "question: 615+531  pred: 1179 \u001b[91m☒\u001b[0m target: 1146\n",
      "question: 609+101  pred: 0659 \u001b[91m☒\u001b[0m target: 0710\n",
      "question: 672+006  pred: 0655 \u001b[91m☒\u001b[0m target: 0678\n",
      "question: 852+076  pred: 0955 \u001b[91m☒\u001b[0m target: 0928\n",
      "question: 821+092  pred: 0855 \u001b[91m☒\u001b[0m target: 0913\n",
      "question: 124+008  pred: 0127 \u001b[91m☒\u001b[0m target: 0132\n",
      "question: 788-692  pred: 0222 \u001b[91m☒\u001b[0m target: 0096\n",
      "question: 626-024  pred: 0655 \u001b[91m☒\u001b[0m target: 0602\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 1 ******************************\n",
      "loss: 0.044815320521593094\n",
      "accuracy: 0.0234375\n",
      "question: 541-052  pred: 0493 \u001b[91m☒\u001b[0m target: 0489\n",
      "question: 743-098  pred: 0623 \u001b[91m☒\u001b[0m target: 0645\n",
      "question: 686+467  pred: 1233 \u001b[91m☒\u001b[0m target: 1153\n",
      "question: 665+052  pred: 0722 \u001b[91m☒\u001b[0m target: 0717\n",
      "question: 481+050  pred: 0599 \u001b[91m☒\u001b[0m target: 0531\n",
      "question: 512+058  pred: 0593 \u001b[91m☒\u001b[0m target: 0570\n",
      "question: 649-020  pred: 0629 \u001b[92m☑\u001b[0m target: 0629\n",
      "question: 848+081  pred: 0932 \u001b[91m☒\u001b[0m target: 0929\n",
      "question: 115+046  pred: 0131 \u001b[91m☒\u001b[0m target: 0161\n",
      "question: 859+077  pred: 0933 \u001b[91m☒\u001b[0m target: 0936\n",
      "--------------------------------------------------\n",
      "loss: 0.04289476200938225\n",
      "accuracy: 0.015625\n",
      "question: 067-003  pred: 0065 \u001b[91m☒\u001b[0m target: 0064\n",
      "question: 579+171  pred: 0727 \u001b[91m☒\u001b[0m target: 0750\n",
      "question: 857-035  pred: 0839 \u001b[91m☒\u001b[0m target: 0822\n",
      "question: 322+066  pred: 0347 \u001b[91m☒\u001b[0m target: 0388\n",
      "question: 698+074  pred: 0724 \u001b[91m☒\u001b[0m target: 0772\n",
      "question: 350+044  pred: 0444 \u001b[91m☒\u001b[0m target: 0394\n",
      "question: 426+059  pred: 0445 \u001b[91m☒\u001b[0m target: 0485\n",
      "question: 092+063  pred: 0151 \u001b[91m☒\u001b[0m target: 0155\n",
      "question: 617-058  pred: 0595 \u001b[91m☒\u001b[0m target: 0559\n",
      "question: 235-037  pred: 0217 \u001b[91m☒\u001b[0m target: 0198\n",
      "--------------------------------------------------\n",
      "loss: 0.037781551480293274\n",
      "accuracy: 0.03125\n",
      "question: 784-035  pred: 0758 \u001b[91m☒\u001b[0m target: 0749\n",
      "question: 665-011  pred: 0666 \u001b[91m☒\u001b[0m target: 0654\n",
      "question: 442-006  pred: 0448 \u001b[91m☒\u001b[0m target: 0436\n",
      "question: 111+090  pred: 0151 \u001b[91m☒\u001b[0m target: 0201\n",
      "question: 025+006  pred: 0028 \u001b[91m☒\u001b[0m target: 0031\n",
      "question: 300+006  pred: 0305 \u001b[91m☒\u001b[0m target: 0306\n",
      "question: 249-005  pred: 0254 \u001b[91m☒\u001b[0m target: 0244\n",
      "question: 262-075  pred: 0108 \u001b[91m☒\u001b[0m target: 0187\n",
      "question: 675-030  pred: 0655 \u001b[91m☒\u001b[0m target: 0645\n",
      "question: 981+149  pred: 1043 \u001b[91m☒\u001b[0m target: 1130\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 2 ******************************\n",
      "loss: 0.033214788883924484\n",
      "accuracy: 0.2109375\n",
      "question: 964+097  pred: 1078 \u001b[91m☒\u001b[0m target: 1061\n",
      "question: 407-007  pred: 0302 \u001b[91m☒\u001b[0m target: 0400\n",
      "question: 769-095  pred: 0674 \u001b[92m☑\u001b[0m target: 0674\n",
      "question: 446-009  pred: 0437 \u001b[92m☑\u001b[0m target: 0437\n",
      "question: 060+023  pred: 0084 \u001b[91m☒\u001b[0m target: 0083\n",
      "question: 874-496  pred: 0388 \u001b[91m☒\u001b[0m target: 0378\n",
      "question: 922+530  pred: 1461 \u001b[91m☒\u001b[0m target: 1452\n",
      "question: 633-026  pred: 0598 \u001b[91m☒\u001b[0m target: 0607\n",
      "question: 586-052  pred: 0534 \u001b[92m☑\u001b[0m target: 0534\n",
      "question: 506-306  pred: 0270 \u001b[91m☒\u001b[0m target: 0200\n",
      "--------------------------------------------------\n",
      "loss: 0.0218451377004385\n",
      "accuracy: 0.3515625\n",
      "question: 929+067  pred: 1096 \u001b[91m☒\u001b[0m target: 0996\n",
      "question: 688-074  pred: 0614 \u001b[92m☑\u001b[0m target: 0614\n",
      "question: 936+761  pred: 1697 \u001b[92m☑\u001b[0m target: 1697\n",
      "question: 892+069  pred: 0961 \u001b[92m☑\u001b[0m target: 0961\n",
      "question: 474-452  pred: 0052 \u001b[91m☒\u001b[0m target: 0022\n",
      "question: 092+029  pred: 0111 \u001b[91m☒\u001b[0m target: 0121\n",
      "question: 581+077  pred: 0668 \u001b[91m☒\u001b[0m target: 0658\n",
      "question: 698-021  pred: 0677 \u001b[92m☑\u001b[0m target: 0677\n",
      "question: 633+047  pred: 0670 \u001b[91m☒\u001b[0m target: 0680\n",
      "question: 745-003  pred: 0742 \u001b[92m☑\u001b[0m target: 0742\n",
      "--------------------------------------------------\n",
      "loss: 0.012766361236572266\n",
      "accuracy: 0.6640625\n",
      "question: 693-063  pred: 0620 \u001b[91m☒\u001b[0m target: 0630\n",
      "question: 080-027  pred: 0043 \u001b[91m☒\u001b[0m target: 0053\n",
      "question: 788-284  pred: 0504 \u001b[92m☑\u001b[0m target: 0504\n",
      "question: 938-011  pred: 0927 \u001b[92m☑\u001b[0m target: 0927\n",
      "question: 581+476  pred: 1057 \u001b[92m☑\u001b[0m target: 1057\n",
      "question: 031-002  pred: 0039 \u001b[91m☒\u001b[0m target: 0029\n",
      "question: 085+030  pred: 0115 \u001b[92m☑\u001b[0m target: 0115\n",
      "question: 539-005  pred: 0534 \u001b[92m☑\u001b[0m target: 0534\n",
      "question: 376+355  pred: 0731 \u001b[92m☑\u001b[0m target: 0731\n",
      "question: 492-092  pred: 0400 \u001b[92m☑\u001b[0m target: 0400\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 3 ******************************\n",
      "loss: 0.00994961243122816\n",
      "accuracy: 0.734375\n",
      "question: 829-748  pred: 0071 \u001b[91m☒\u001b[0m target: 0081\n",
      "question: 749-006  pred: 0743 \u001b[92m☑\u001b[0m target: 0743\n",
      "question: 254-101  pred: 0153 \u001b[92m☑\u001b[0m target: 0153\n",
      "question: 985+190  pred: 1175 \u001b[92m☑\u001b[0m target: 1175\n",
      "question: 589+059  pred: 0648 \u001b[92m☑\u001b[0m target: 0648\n",
      "question: 333-021  pred: 0312 \u001b[92m☑\u001b[0m target: 0312\n",
      "question: 562+076  pred: 0638 \u001b[92m☑\u001b[0m target: 0638\n",
      "question: 792+056  pred: 0848 \u001b[92m☑\u001b[0m target: 0848\n",
      "question: 088-019  pred: 0059 \u001b[91m☒\u001b[0m target: 0069\n",
      "question: 493+079  pred: 0572 \u001b[92m☑\u001b[0m target: 0572\n",
      "--------------------------------------------------\n",
      "loss: 0.005345574114471674\n",
      "accuracy: 0.875\n",
      "question: 637+094  pred: 0731 \u001b[92m☑\u001b[0m target: 0731\n",
      "question: 480+022  pred: 0502 \u001b[92m☑\u001b[0m target: 0502\n",
      "question: 056-024  pred: 0032 \u001b[92m☑\u001b[0m target: 0032\n",
      "question: 871+004  pred: 0875 \u001b[92m☑\u001b[0m target: 0875\n",
      "question: 070+027  pred: 0097 \u001b[92m☑\u001b[0m target: 0097\n",
      "question: 770+002  pred: 0772 \u001b[92m☑\u001b[0m target: 0772\n",
      "question: 807+779  pred: 1686 \u001b[91m☒\u001b[0m target: 1586\n",
      "question: 731+005  pred: 0736 \u001b[92m☑\u001b[0m target: 0736\n",
      "question: 942-008  pred: 0934 \u001b[92m☑\u001b[0m target: 0934\n",
      "question: 934-076  pred: 0858 \u001b[92m☑\u001b[0m target: 0858\n",
      "--------------------------------------------------\n",
      "loss: 0.004590683151036501\n",
      "accuracy: 0.90625\n",
      "question: 081+014  pred: 0095 \u001b[92m☑\u001b[0m target: 0095\n",
      "question: 990+071  pred: 1061 \u001b[92m☑\u001b[0m target: 1061\n",
      "question: 091-006  pred: 0085 \u001b[92m☑\u001b[0m target: 0085\n",
      "question: 292+226  pred: 0428 \u001b[91m☒\u001b[0m target: 0518\n",
      "question: 729-004  pred: 0725 \u001b[92m☑\u001b[0m target: 0725\n",
      "question: 888+011  pred: 0899 \u001b[92m☑\u001b[0m target: 0899\n",
      "question: 109-096  pred: 0013 \u001b[92m☑\u001b[0m target: 0013\n",
      "question: 606-393  pred: 0213 \u001b[92m☑\u001b[0m target: 0213\n",
      "question: 950-190  pred: 0750 \u001b[91m☒\u001b[0m target: 0760\n",
      "question: 311+081  pred: 0392 \u001b[92m☑\u001b[0m target: 0392\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 4 ******************************\n",
      "loss: 0.0023940938990563154\n",
      "accuracy: 0.953125\n",
      "question: 538+009  pred: 0547 \u001b[92m☑\u001b[0m target: 0547\n",
      "question: 928+842  pred: 1770 \u001b[92m☑\u001b[0m target: 1770\n",
      "question: 893-087  pred: 0806 \u001b[92m☑\u001b[0m target: 0806\n",
      "question: 181-028  pred: 0153 \u001b[92m☑\u001b[0m target: 0153\n",
      "question: 232-077  pred: 0155 \u001b[92m☑\u001b[0m target: 0155\n",
      "question: 913-058  pred: 0855 \u001b[92m☑\u001b[0m target: 0855\n",
      "question: 544-096  pred: 0448 \u001b[92m☑\u001b[0m target: 0448\n",
      "question: 950-009  pred: 0941 \u001b[92m☑\u001b[0m target: 0941\n",
      "question: 864+039  pred: 0903 \u001b[92m☑\u001b[0m target: 0903\n",
      "question: 883+045  pred: 0938 \u001b[91m☒\u001b[0m target: 0928\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0016267775790765882\n",
      "accuracy: 0.96875\n",
      "question: 551+009  pred: 0560 \u001b[92m☑\u001b[0m target: 0560\n",
      "question: 866-003  pred: 0863 \u001b[92m☑\u001b[0m target: 0863\n",
      "question: 502-007  pred: 0495 \u001b[92m☑\u001b[0m target: 0495\n",
      "question: 651+048  pred: 0699 \u001b[92m☑\u001b[0m target: 0699\n",
      "question: 535+037  pred: 0572 \u001b[92m☑\u001b[0m target: 0572\n",
      "question: 017-001  pred: 0016 \u001b[92m☑\u001b[0m target: 0016\n",
      "question: 079-009  pred: 0070 \u001b[92m☑\u001b[0m target: 0070\n",
      "question: 942+075  pred: 1017 \u001b[92m☑\u001b[0m target: 1017\n",
      "question: 890+060  pred: 0950 \u001b[92m☑\u001b[0m target: 0950\n",
      "question: 736-259  pred: 0477 \u001b[92m☑\u001b[0m target: 0477\n",
      "--------------------------------------------------\n",
      "loss: 0.0010746417101472616\n",
      "accuracy: 0.9921875\n",
      "question: 314+081  pred: 0395 \u001b[92m☑\u001b[0m target: 0395\n",
      "question: 622-015  pred: 0607 \u001b[92m☑\u001b[0m target: 0607\n",
      "question: 052-031  pred: 0021 \u001b[92m☑\u001b[0m target: 0021\n",
      "question: 198-061  pred: 0137 \u001b[92m☑\u001b[0m target: 0137\n",
      "question: 812-007  pred: 0805 \u001b[92m☑\u001b[0m target: 0805\n",
      "question: 114+078  pred: 0192 \u001b[92m☑\u001b[0m target: 0192\n",
      "question: 104+099  pred: 0203 \u001b[92m☑\u001b[0m target: 0203\n",
      "question: 655+006  pred: 0661 \u001b[92m☑\u001b[0m target: 0661\n",
      "question: 423+036  pred: 0459 \u001b[92m☑\u001b[0m target: 0459\n",
      "question: 889+074  pred: 0963 \u001b[92m☑\u001b[0m target: 0963\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 5 ******************************\n",
      "loss: 0.000555755861569196\n",
      "accuracy: 1.0\n",
      "question: 562-016  pred: 0546 \u001b[92m☑\u001b[0m target: 0546\n",
      "question: 431-028  pred: 0403 \u001b[92m☑\u001b[0m target: 0403\n",
      "question: 080+073  pred: 0153 \u001b[92m☑\u001b[0m target: 0153\n",
      "question: 737+637  pred: 1374 \u001b[92m☑\u001b[0m target: 1374\n",
      "question: 551+077  pred: 0628 \u001b[92m☑\u001b[0m target: 0628\n",
      "question: 819-009  pred: 0810 \u001b[92m☑\u001b[0m target: 0810\n",
      "question: 464-024  pred: 0440 \u001b[92m☑\u001b[0m target: 0440\n",
      "question: 794-078  pred: 0716 \u001b[92m☑\u001b[0m target: 0716\n",
      "question: 505+188  pred: 0693 \u001b[92m☑\u001b[0m target: 0693\n",
      "question: 440-236  pred: 0204 \u001b[92m☑\u001b[0m target: 0204\n",
      "--------------------------------------------------\n",
      "loss: 0.0004448329855222255\n",
      "accuracy: 0.9921875\n",
      "question: 301-050  pred: 0251 \u001b[92m☑\u001b[0m target: 0251\n",
      "question: 893+065  pred: 0958 \u001b[92m☑\u001b[0m target: 0958\n",
      "question: 136+023  pred: 0159 \u001b[92m☑\u001b[0m target: 0159\n",
      "question: 724-085  pred: 0639 \u001b[92m☑\u001b[0m target: 0639\n",
      "question: 085+068  pred: 0153 \u001b[92m☑\u001b[0m target: 0153\n",
      "question: 308-071  pred: 0237 \u001b[92m☑\u001b[0m target: 0237\n",
      "question: 612+086  pred: 0698 \u001b[92m☑\u001b[0m target: 0698\n",
      "question: 652-336  pred: 0316 \u001b[92m☑\u001b[0m target: 0316\n",
      "question: 081-067  pred: 0014 \u001b[92m☑\u001b[0m target: 0014\n",
      "question: 325-054  pred: 0271 \u001b[92m☑\u001b[0m target: 0271\n",
      "--------------------------------------------------\n",
      "loss: 0.00022409310622606426\n",
      "accuracy: 0.9921875\n",
      "question: 321-098  pred: 0223 \u001b[92m☑\u001b[0m target: 0223\n",
      "question: 354+205  pred: 0559 \u001b[92m☑\u001b[0m target: 0559\n",
      "question: 947+001  pred: 0948 \u001b[92m☑\u001b[0m target: 0948\n",
      "question: 291-014  pred: 0277 \u001b[92m☑\u001b[0m target: 0277\n",
      "question: 277-075  pred: 0202 \u001b[92m☑\u001b[0m target: 0202\n",
      "question: 858+083  pred: 0941 \u001b[92m☑\u001b[0m target: 0941\n",
      "question: 109-007  pred: 0102 \u001b[92m☑\u001b[0m target: 0102\n",
      "question: 322-006  pred: 0316 \u001b[92m☑\u001b[0m target: 0316\n",
      "question: 225-146  pred: 0079 \u001b[92m☑\u001b[0m target: 0079\n",
      "question: 254-010  pred: 0244 \u001b[92m☑\u001b[0m target: 0244\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 6 ******************************\n",
      "loss: 0.00031839506118558347\n",
      "accuracy: 1.0\n",
      "question: 196+029  pred: 0225 \u001b[92m☑\u001b[0m target: 0225\n",
      "question: 861-012  pred: 0849 \u001b[92m☑\u001b[0m target: 0849\n",
      "question: 039+002  pred: 0041 \u001b[92m☑\u001b[0m target: 0041\n",
      "question: 291+007  pred: 0298 \u001b[92m☑\u001b[0m target: 0298\n",
      "question: 505+050  pred: 0555 \u001b[92m☑\u001b[0m target: 0555\n",
      "question: 584-396  pred: 0188 \u001b[92m☑\u001b[0m target: 0188\n",
      "question: 566+144  pred: 0710 \u001b[92m☑\u001b[0m target: 0710\n",
      "question: 816+096  pred: 0912 \u001b[92m☑\u001b[0m target: 0912\n",
      "question: 209+013  pred: 0222 \u001b[92m☑\u001b[0m target: 0222\n",
      "question: 857-427  pred: 0430 \u001b[92m☑\u001b[0m target: 0430\n",
      "--------------------------------------------------\n",
      "loss: 0.0009315226343460381\n",
      "accuracy: 0.9921875\n",
      "question: 840-622  pred: 0218 \u001b[92m☑\u001b[0m target: 0218\n",
      "question: 031-031  pred: 0000 \u001b[92m☑\u001b[0m target: 0000\n",
      "question: 273+093  pred: 0366 \u001b[92m☑\u001b[0m target: 0366\n",
      "question: 040-029  pred: 0011 \u001b[92m☑\u001b[0m target: 0011\n",
      "question: 701+376  pred: 1077 \u001b[92m☑\u001b[0m target: 1077\n",
      "question: 094-050  pred: 0044 \u001b[92m☑\u001b[0m target: 0044\n",
      "question: 333-080  pred: 0253 \u001b[92m☑\u001b[0m target: 0253\n",
      "question: 116+017  pred: 0133 \u001b[92m☑\u001b[0m target: 0133\n",
      "question: 149+067  pred: 0216 \u001b[92m☑\u001b[0m target: 0216\n",
      "question: 653-021  pred: 0632 \u001b[92m☑\u001b[0m target: 0632\n",
      "--------------------------------------------------\n",
      "loss: 0.002808883786201477\n",
      "accuracy: 0.921875\n",
      "question: 304-076  pred: 0228 \u001b[92m☑\u001b[0m target: 0228\n",
      "question: 829+004  pred: 0833 \u001b[92m☑\u001b[0m target: 0833\n",
      "question: 968-057  pred: 0911 \u001b[92m☑\u001b[0m target: 0911\n",
      "question: 989-013  pred: 0976 \u001b[92m☑\u001b[0m target: 0976\n",
      "question: 511-045  pred: 0466 \u001b[92m☑\u001b[0m target: 0466\n",
      "question: 322+092  pred: 0414 \u001b[92m☑\u001b[0m target: 0414\n",
      "question: 842-017  pred: 0826 \u001b[91m☒\u001b[0m target: 0825\n",
      "question: 246-007  pred: 0239 \u001b[92m☑\u001b[0m target: 0239\n",
      "question: 808-047  pred: 0762 \u001b[91m☒\u001b[0m target: 0761\n",
      "question: 275-203  pred: 0072 \u001b[92m☑\u001b[0m target: 0072\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 7 ******************************\n",
      "loss: 0.0012107923394069076\n",
      "accuracy: 0.96875\n",
      "question: 559+000  pred: 0559 \u001b[92m☑\u001b[0m target: 0559\n",
      "question: 510+031  pred: 0541 \u001b[92m☑\u001b[0m target: 0541\n",
      "question: 478-033  pred: 0445 \u001b[92m☑\u001b[0m target: 0445\n",
      "question: 172-092  pred: 0080 \u001b[92m☑\u001b[0m target: 0080\n",
      "question: 488-160  pred: 0328 \u001b[92m☑\u001b[0m target: 0328\n",
      "question: 517-018  pred: 0499 \u001b[92m☑\u001b[0m target: 0499\n",
      "question: 234-055  pred: 0179 \u001b[92m☑\u001b[0m target: 0179\n",
      "question: 590+008  pred: 0598 \u001b[92m☑\u001b[0m target: 0598\n",
      "question: 905-897  pred: 0008 \u001b[92m☑\u001b[0m target: 0008\n",
      "question: 110-055  pred: 0055 \u001b[92m☑\u001b[0m target: 0055\n",
      "--------------------------------------------------\n",
      "loss: 0.0006916855345480144\n",
      "accuracy: 0.9921875\n",
      "question: 534-084  pred: 0450 \u001b[92m☑\u001b[0m target: 0450\n",
      "question: 070+036  pred: 0106 \u001b[92m☑\u001b[0m target: 0106\n",
      "question: 990+620  pred: 1610 \u001b[92m☑\u001b[0m target: 1610\n",
      "question: 072-064  pred: 0008 \u001b[92m☑\u001b[0m target: 0008\n",
      "question: 115+006  pred: 0121 \u001b[92m☑\u001b[0m target: 0121\n",
      "question: 296+259  pred: 0555 \u001b[92m☑\u001b[0m target: 0555\n",
      "question: 567+004  pred: 0571 \u001b[92m☑\u001b[0m target: 0571\n",
      "question: 746+010  pred: 0756 \u001b[92m☑\u001b[0m target: 0756\n",
      "question: 458-001  pred: 0457 \u001b[92m☑\u001b[0m target: 0457\n",
      "question: 622-015  pred: 0607 \u001b[92m☑\u001b[0m target: 0607\n",
      "--------------------------------------------------\n",
      "loss: 0.00026394150336273015\n",
      "accuracy: 1.0\n",
      "question: 981+008  pred: 0989 \u001b[92m☑\u001b[0m target: 0989\n",
      "question: 871+838  pred: 1709 \u001b[92m☑\u001b[0m target: 1709\n",
      "question: 342+009  pred: 0351 \u001b[92m☑\u001b[0m target: 0351\n",
      "question: 421+252  pred: 0673 \u001b[92m☑\u001b[0m target: 0673\n",
      "question: 504+035  pred: 0539 \u001b[92m☑\u001b[0m target: 0539\n",
      "question: 679-042  pred: 0637 \u001b[92m☑\u001b[0m target: 0637\n",
      "question: 937+008  pred: 0945 \u001b[92m☑\u001b[0m target: 0945\n",
      "question: 604-039  pred: 0565 \u001b[92m☑\u001b[0m target: 0565\n",
      "question: 081+064  pred: 0145 \u001b[92m☑\u001b[0m target: 0145\n",
      "question: 206+044  pred: 0250 \u001b[92m☑\u001b[0m target: 0250\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 8 ******************************\n",
      "loss: 0.00015960192831698805\n",
      "accuracy: 1.0\n",
      "question: 074-033  pred: 0041 \u001b[92m☑\u001b[0m target: 0041\n",
      "question: 084+009  pred: 0093 \u001b[92m☑\u001b[0m target: 0093\n",
      "question: 708+007  pred: 0715 \u001b[92m☑\u001b[0m target: 0715\n",
      "question: 909-311  pred: 0598 \u001b[92m☑\u001b[0m target: 0598\n",
      "question: 815+129  pred: 0944 \u001b[92m☑\u001b[0m target: 0944\n",
      "question: 940+047  pred: 0987 \u001b[92m☑\u001b[0m target: 0987\n",
      "question: 289-007  pred: 0282 \u001b[92m☑\u001b[0m target: 0282\n",
      "question: 682-000  pred: 0682 \u001b[92m☑\u001b[0m target: 0682\n",
      "question: 199+069  pred: 0268 \u001b[92m☑\u001b[0m target: 0268\n",
      "question: 142+096  pred: 0238 \u001b[92m☑\u001b[0m target: 0238\n",
      "--------------------------------------------------\n",
      "loss: 0.00019570345466490835\n",
      "accuracy: 0.9921875\n",
      "question: 092+052  pred: 0144 \u001b[92m☑\u001b[0m target: 0144\n",
      "question: 524+051  pred: 0575 \u001b[92m☑\u001b[0m target: 0575\n",
      "question: 495+064  pred: 0559 \u001b[92m☑\u001b[0m target: 0559\n",
      "question: 311-002  pred: 0309 \u001b[92m☑\u001b[0m target: 0309\n",
      "question: 950+019  pred: 0969 \u001b[92m☑\u001b[0m target: 0969\n",
      "question: 886+043  pred: 0929 \u001b[92m☑\u001b[0m target: 0929\n",
      "question: 514-002  pred: 0512 \u001b[92m☑\u001b[0m target: 0512\n",
      "question: 324-000  pred: 0324 \u001b[92m☑\u001b[0m target: 0324\n",
      "question: 866+443  pred: 1309 \u001b[92m☑\u001b[0m target: 1309\n",
      "question: 109-004  pred: 0105 \u001b[92m☑\u001b[0m target: 0105\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.009166827425361e-05\n",
      "accuracy: 1.0\n",
      "question: 384-006  pred: 0378 \u001b[92m☑\u001b[0m target: 0378\n",
      "question: 061-061  pred: 0000 \u001b[92m☑\u001b[0m target: 0000\n",
      "question: 504+009  pred: 0513 \u001b[92m☑\u001b[0m target: 0513\n",
      "question: 166-066  pred: 0100 \u001b[92m☑\u001b[0m target: 0100\n",
      "question: 279-004  pred: 0275 \u001b[92m☑\u001b[0m target: 0275\n",
      "question: 551-067  pred: 0484 \u001b[92m☑\u001b[0m target: 0484\n",
      "question: 629+078  pred: 0707 \u001b[92m☑\u001b[0m target: 0707\n",
      "question: 842+528  pred: 1370 \u001b[92m☑\u001b[0m target: 1370\n",
      "question: 094-041  pred: 0053 \u001b[92m☑\u001b[0m target: 0053\n",
      "question: 588+051  pred: 0639 \u001b[92m☑\u001b[0m target: 0639\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 9 ******************************\n",
      "loss: 7.71996274124831e-05\n",
      "accuracy: 1.0\n",
      "question: 941-015  pred: 0926 \u001b[92m☑\u001b[0m target: 0926\n",
      "question: 535-000  pred: 0535 \u001b[92m☑\u001b[0m target: 0535\n",
      "question: 449-046  pred: 0403 \u001b[92m☑\u001b[0m target: 0403\n",
      "question: 075-006  pred: 0069 \u001b[92m☑\u001b[0m target: 0069\n",
      "question: 989-052  pred: 0937 \u001b[92m☑\u001b[0m target: 0937\n",
      "question: 124+043  pred: 0167 \u001b[92m☑\u001b[0m target: 0167\n",
      "question: 293-015  pred: 0278 \u001b[92m☑\u001b[0m target: 0278\n",
      "question: 753+080  pred: 0833 \u001b[92m☑\u001b[0m target: 0833\n",
      "question: 217+052  pred: 0269 \u001b[92m☑\u001b[0m target: 0269\n",
      "question: 570+058  pred: 0628 \u001b[92m☑\u001b[0m target: 0628\n",
      "--------------------------------------------------\n",
      "loss: 0.002031446900218725\n",
      "accuracy: 0.953125\n",
      "question: 063+062  pred: 0125 \u001b[92m☑\u001b[0m target: 0125\n",
      "question: 716+050  pred: 0766 \u001b[92m☑\u001b[0m target: 0766\n",
      "question: 344+086  pred: 0430 \u001b[92m☑\u001b[0m target: 0430\n",
      "question: 094-004  pred: 0090 \u001b[92m☑\u001b[0m target: 0090\n",
      "question: 247-079  pred: 0168 \u001b[92m☑\u001b[0m target: 0168\n",
      "question: 023+013  pred: 0036 \u001b[92m☑\u001b[0m target: 0036\n",
      "question: 184-016  pred: 0168 \u001b[92m☑\u001b[0m target: 0168\n",
      "question: 825-050  pred: 0775 \u001b[92m☑\u001b[0m target: 0775\n",
      "question: 445-088  pred: 0357 \u001b[92m☑\u001b[0m target: 0357\n",
      "question: 756+036  pred: 0792 \u001b[92m☑\u001b[0m target: 0792\n",
      "--------------------------------------------------\n",
      "loss: 0.0012503801845014095\n",
      "accuracy: 0.96875\n",
      "question: 764-444  pred: 0320 \u001b[92m☑\u001b[0m target: 0320\n",
      "question: 802+619  pred: 1421 \u001b[92m☑\u001b[0m target: 1421\n",
      "question: 715+086  pred: 0801 \u001b[92m☑\u001b[0m target: 0801\n",
      "question: 869+429  pred: 1298 \u001b[92m☑\u001b[0m target: 1298\n",
      "question: 929+015  pred: 0944 \u001b[92m☑\u001b[0m target: 0944\n",
      "question: 989+093  pred: 1082 \u001b[92m☑\u001b[0m target: 1082\n",
      "question: 337-067  pred: 0270 \u001b[92m☑\u001b[0m target: 0270\n",
      "question: 351-038  pred: 0313 \u001b[92m☑\u001b[0m target: 0313\n",
      "question: 191-007  pred: 0184 \u001b[92m☑\u001b[0m target: 0184\n",
      "question: 671+416  pred: 1087 \u001b[92m☑\u001b[0m target: 1087\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 10 ******************************\n",
      "loss: 0.0009732469916343689\n",
      "accuracy: 0.9765625\n",
      "question: 902+628  pred: 1530 \u001b[92m☑\u001b[0m target: 1530\n",
      "question: 446-001  pred: 0445 \u001b[92m☑\u001b[0m target: 0445\n",
      "question: 942+029  pred: 0971 \u001b[92m☑\u001b[0m target: 0971\n",
      "question: 400-062  pred: 0338 \u001b[92m☑\u001b[0m target: 0338\n",
      "question: 895-311  pred: 0584 \u001b[92m☑\u001b[0m target: 0584\n",
      "question: 869-002  pred: 0867 \u001b[92m☑\u001b[0m target: 0867\n",
      "question: 985-001  pred: 0984 \u001b[92m☑\u001b[0m target: 0984\n",
      "question: 325-002  pred: 0323 \u001b[92m☑\u001b[0m target: 0323\n",
      "question: 868+000  pred: 0868 \u001b[92m☑\u001b[0m target: 0868\n",
      "question: 680+096  pred: 0776 \u001b[92m☑\u001b[0m target: 0776\n",
      "--------------------------------------------------\n",
      "loss: 0.002198903588578105\n",
      "accuracy: 0.953125\n",
      "question: 634+010  pred: 0644 \u001b[92m☑\u001b[0m target: 0644\n",
      "question: 427-003  pred: 0424 \u001b[92m☑\u001b[0m target: 0424\n",
      "question: 798+591  pred: 1389 \u001b[92m☑\u001b[0m target: 1389\n",
      "question: 027-011  pred: 0016 \u001b[92m☑\u001b[0m target: 0016\n",
      "question: 399+036  pred: 0435 \u001b[92m☑\u001b[0m target: 0435\n",
      "question: 920-029  pred: 0891 \u001b[92m☑\u001b[0m target: 0891\n",
      "question: 248-021  pred: 0227 \u001b[92m☑\u001b[0m target: 0227\n",
      "question: 955+180  pred: 1135 \u001b[92m☑\u001b[0m target: 1135\n",
      "question: 865+073  pred: 0938 \u001b[92m☑\u001b[0m target: 0938\n",
      "question: 800+060  pred: 0760 \u001b[91m☒\u001b[0m target: 0860\n",
      "--------------------------------------------------\n",
      "loss: 0.0009462549351155758\n",
      "accuracy: 0.9765625\n",
      "question: 842-017  pred: 0825 \u001b[92m☑\u001b[0m target: 0825\n",
      "question: 252-154  pred: 0098 \u001b[92m☑\u001b[0m target: 0098\n",
      "question: 297-046  pred: 0251 \u001b[92m☑\u001b[0m target: 0251\n",
      "question: 375-069  pred: 0306 \u001b[92m☑\u001b[0m target: 0306\n",
      "question: 790+239  pred: 1029 \u001b[92m☑\u001b[0m target: 1029\n",
      "question: 649+242  pred: 0881 \u001b[91m☒\u001b[0m target: 0891\n",
      "question: 883+007  pred: 0890 \u001b[92m☑\u001b[0m target: 0890\n",
      "question: 662-559  pred: 0103 \u001b[92m☑\u001b[0m target: 0103\n",
      "question: 105+093  pred: 0198 \u001b[92m☑\u001b[0m target: 0198\n",
      "question: 802+004  pred: 0806 \u001b[92m☑\u001b[0m target: 0806\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 11 ******************************\n",
      "loss: 0.00015313310723286122\n",
      "accuracy: 1.0\n",
      "question: 347-079  pred: 0268 \u001b[92m☑\u001b[0m target: 0268\n",
      "question: 074+031  pred: 0105 \u001b[92m☑\u001b[0m target: 0105\n",
      "question: 813-124  pred: 0689 \u001b[92m☑\u001b[0m target: 0689\n",
      "question: 711+003  pred: 0714 \u001b[92m☑\u001b[0m target: 0714\n",
      "question: 243+017  pred: 0260 \u001b[92m☑\u001b[0m target: 0260\n",
      "question: 995-752  pred: 0243 \u001b[92m☑\u001b[0m target: 0243\n",
      "question: 099-064  pred: 0035 \u001b[92m☑\u001b[0m target: 0035\n",
      "question: 084+045  pred: 0129 \u001b[92m☑\u001b[0m target: 0129\n",
      "question: 582-416  pred: 0166 \u001b[92m☑\u001b[0m target: 0166\n",
      "question: 845+009  pred: 0854 \u001b[92m☑\u001b[0m target: 0854\n",
      "--------------------------------------------------\n",
      "loss: 0.00020641247101593763\n",
      "accuracy: 1.0\n",
      "question: 156-005  pred: 0151 \u001b[92m☑\u001b[0m target: 0151\n",
      "question: 032-009  pred: 0023 \u001b[92m☑\u001b[0m target: 0023\n",
      "question: 937-515  pred: 0422 \u001b[92m☑\u001b[0m target: 0422\n",
      "question: 978+570  pred: 1548 \u001b[92m☑\u001b[0m target: 1548\n",
      "question: 186+040  pred: 0226 \u001b[92m☑\u001b[0m target: 0226\n",
      "question: 761+000  pred: 0761 \u001b[92m☑\u001b[0m target: 0761\n",
      "question: 407-001  pred: 0406 \u001b[92m☑\u001b[0m target: 0406\n",
      "question: 344-084  pred: 0260 \u001b[92m☑\u001b[0m target: 0260\n",
      "question: 539+038  pred: 0577 \u001b[92m☑\u001b[0m target: 0577\n",
      "question: 821+098  pred: 0919 \u001b[92m☑\u001b[0m target: 0919\n",
      "--------------------------------------------------\n",
      "loss: 0.0007398347370326519\n",
      "accuracy: 0.984375\n",
      "question: 683-649  pred: 0034 \u001b[92m☑\u001b[0m target: 0034\n",
      "question: 655+008  pred: 0663 \u001b[92m☑\u001b[0m target: 0663\n",
      "question: 469-041  pred: 0428 \u001b[92m☑\u001b[0m target: 0428\n",
      "question: 938-449  pred: 0489 \u001b[92m☑\u001b[0m target: 0489\n",
      "question: 280+000  pred: 0280 \u001b[92m☑\u001b[0m target: 0280\n",
      "question: 640-027  pred: 0613 \u001b[92m☑\u001b[0m target: 0613\n",
      "question: 914+031  pred: 0945 \u001b[92m☑\u001b[0m target: 0945\n",
      "question: 033+023  pred: 0056 \u001b[92m☑\u001b[0m target: 0056\n",
      "question: 863-099  pred: 0764 \u001b[92m☑\u001b[0m target: 0764\n",
      "question: 644+023  pred: 0667 \u001b[92m☑\u001b[0m target: 0667\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 12 ******************************\n",
      "loss: 0.00013301311992108822\n",
      "accuracy: 1.0\n",
      "question: 622-098  pred: 0524 \u001b[92m☑\u001b[0m target: 0524\n",
      "question: 897-075  pred: 0822 \u001b[92m☑\u001b[0m target: 0822\n",
      "question: 937-005  pred: 0932 \u001b[92m☑\u001b[0m target: 0932\n",
      "question: 052+034  pred: 0086 \u001b[92m☑\u001b[0m target: 0086\n",
      "question: 575+005  pred: 0580 \u001b[92m☑\u001b[0m target: 0580\n",
      "question: 805+077  pred: 0882 \u001b[92m☑\u001b[0m target: 0882\n",
      "question: 844-028  pred: 0816 \u001b[92m☑\u001b[0m target: 0816\n",
      "question: 203-098  pred: 0105 \u001b[92m☑\u001b[0m target: 0105\n",
      "question: 040+008  pred: 0048 \u001b[92m☑\u001b[0m target: 0048\n",
      "question: 244+043  pred: 0287 \u001b[92m☑\u001b[0m target: 0287\n",
      "--------------------------------------------------\n",
      "loss: 0.00036087201442569494\n",
      "accuracy: 0.9921875\n",
      "question: 735-090  pred: 0645 \u001b[92m☑\u001b[0m target: 0645\n",
      "question: 682-057  pred: 0625 \u001b[92m☑\u001b[0m target: 0625\n",
      "question: 122-078  pred: 0044 \u001b[92m☑\u001b[0m target: 0044\n",
      "question: 930-065  pred: 0865 \u001b[92m☑\u001b[0m target: 0865\n",
      "question: 750-000  pred: 0750 \u001b[92m☑\u001b[0m target: 0750\n",
      "question: 894+262  pred: 1156 \u001b[92m☑\u001b[0m target: 1156\n",
      "question: 192-069  pred: 0123 \u001b[92m☑\u001b[0m target: 0123\n",
      "question: 181-028  pred: 0153 \u001b[92m☑\u001b[0m target: 0153\n",
      "question: 780-643  pred: 0137 \u001b[92m☑\u001b[0m target: 0137\n",
      "question: 845-005  pred: 0840 \u001b[92m☑\u001b[0m target: 0840\n",
      "--------------------------------------------------\n",
      "loss: 0.00011257932055741549\n",
      "accuracy: 1.0\n",
      "question: 403-065  pred: 0338 \u001b[92m☑\u001b[0m target: 0338\n",
      "question: 173-011  pred: 0162 \u001b[92m☑\u001b[0m target: 0162\n",
      "question: 308+283  pred: 0591 \u001b[92m☑\u001b[0m target: 0591\n",
      "question: 485+028  pred: 0513 \u001b[92m☑\u001b[0m target: 0513\n",
      "question: 403+077  pred: 0480 \u001b[92m☑\u001b[0m target: 0480\n",
      "question: 827-005  pred: 0822 \u001b[92m☑\u001b[0m target: 0822\n",
      "question: 415+095  pred: 0510 \u001b[92m☑\u001b[0m target: 0510\n",
      "question: 516-086  pred: 0430 \u001b[92m☑\u001b[0m target: 0430\n",
      "question: 274-062  pred: 0212 \u001b[92m☑\u001b[0m target: 0212\n",
      "question: 982-744  pred: 0238 \u001b[92m☑\u001b[0m target: 0238\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** epoch: 13 ******************************\n",
      "loss: 0.00017289390962105244\n",
      "accuracy: 1.0\n",
      "question: 619+014  pred: 0633 \u001b[92m☑\u001b[0m target: 0633\n",
      "question: 283+047  pred: 0330 \u001b[92m☑\u001b[0m target: 0330\n",
      "question: 567+048  pred: 0615 \u001b[92m☑\u001b[0m target: 0615\n",
      "question: 864-227  pred: 0637 \u001b[92m☑\u001b[0m target: 0637\n",
      "question: 414+077  pred: 0491 \u001b[92m☑\u001b[0m target: 0491\n",
      "question: 044-020  pred: 0024 \u001b[92m☑\u001b[0m target: 0024\n",
      "question: 380-281  pred: 0099 \u001b[92m☑\u001b[0m target: 0099\n",
      "question: 845-004  pred: 0841 \u001b[92m☑\u001b[0m target: 0841\n",
      "question: 718-049  pred: 0669 \u001b[92m☑\u001b[0m target: 0669\n",
      "question: 165-009  pred: 0156 \u001b[92m☑\u001b[0m target: 0156\n",
      "--------------------------------------------------\n",
      "loss: 5.77321152377408e-05\n",
      "accuracy: 1.0\n",
      "question: 786-019  pred: 0767 \u001b[92m☑\u001b[0m target: 0767\n",
      "question: 738+007  pred: 0745 \u001b[92m☑\u001b[0m target: 0745\n",
      "question: 613-392  pred: 0221 \u001b[92m☑\u001b[0m target: 0221\n",
      "question: 093+020  pred: 0113 \u001b[92m☑\u001b[0m target: 0113\n",
      "question: 651-054  pred: 0597 \u001b[92m☑\u001b[0m target: 0597\n",
      "question: 723-028  pred: 0695 \u001b[92m☑\u001b[0m target: 0695\n",
      "question: 499-034  pred: 0465 \u001b[92m☑\u001b[0m target: 0465\n",
      "question: 538-033  pred: 0505 \u001b[92m☑\u001b[0m target: 0505\n",
      "question: 055-033  pred: 0022 \u001b[92m☑\u001b[0m target: 0022\n",
      "question: 058-011  pred: 0047 \u001b[92m☑\u001b[0m target: 0047\n",
      "--------------------------------------------------\n",
      "loss: 4.6003537136130035e-05\n",
      "accuracy: 1.0\n",
      "question: 960-089  pred: 0871 \u001b[92m☑\u001b[0m target: 0871\n",
      "question: 463+008  pred: 0471 \u001b[92m☑\u001b[0m target: 0471\n",
      "question: 867+723  pred: 1590 \u001b[92m☑\u001b[0m target: 1590\n",
      "question: 807-035  pred: 0772 \u001b[92m☑\u001b[0m target: 0772\n",
      "question: 542+005  pred: 0547 \u001b[92m☑\u001b[0m target: 0547\n",
      "question: 122-007  pred: 0115 \u001b[92m☑\u001b[0m target: 0115\n",
      "question: 395-031  pred: 0364 \u001b[92m☑\u001b[0m target: 0364\n",
      "question: 392-000  pred: 0392 \u001b[92m☑\u001b[0m target: 0392\n",
      "question: 868+230  pred: 1098 \u001b[92m☑\u001b[0m target: 1098\n",
      "question: 673+005  pred: 0678 \u001b[92m☑\u001b[0m target: 0678\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 14 ******************************\n",
      "loss: 0.0001107698553823866\n",
      "accuracy: 1.0\n",
      "question: 893-374  pred: 0519 \u001b[92m☑\u001b[0m target: 0519\n",
      "question: 092-036  pred: 0056 \u001b[92m☑\u001b[0m target: 0056\n",
      "question: 232+060  pred: 0292 \u001b[92m☑\u001b[0m target: 0292\n",
      "question: 298-258  pred: 0040 \u001b[92m☑\u001b[0m target: 0040\n",
      "question: 750-081  pred: 0669 \u001b[92m☑\u001b[0m target: 0669\n",
      "question: 451+051  pred: 0502 \u001b[92m☑\u001b[0m target: 0502\n",
      "question: 925+000  pred: 0925 \u001b[92m☑\u001b[0m target: 0925\n",
      "question: 989+086  pred: 1075 \u001b[92m☑\u001b[0m target: 1075\n",
      "question: 104+021  pred: 0125 \u001b[92m☑\u001b[0m target: 0125\n",
      "question: 682-019  pred: 0663 \u001b[92m☑\u001b[0m target: 0663\n",
      "--------------------------------------------------\n",
      "loss: 9.308730113843922e-06\n",
      "accuracy: 1.0\n",
      "question: 529+069  pred: 0598 \u001b[92m☑\u001b[0m target: 0598\n",
      "question: 277-000  pred: 0277 \u001b[92m☑\u001b[0m target: 0277\n",
      "question: 799+603  pred: 1402 \u001b[92m☑\u001b[0m target: 1402\n",
      "question: 528-006  pred: 0522 \u001b[92m☑\u001b[0m target: 0522\n",
      "question: 660-510  pred: 0150 \u001b[92m☑\u001b[0m target: 0150\n",
      "question: 108-007  pred: 0101 \u001b[92m☑\u001b[0m target: 0101\n",
      "question: 114-018  pred: 0096 \u001b[92m☑\u001b[0m target: 0096\n",
      "question: 193-075  pred: 0118 \u001b[92m☑\u001b[0m target: 0118\n",
      "question: 813-006  pred: 0807 \u001b[92m☑\u001b[0m target: 0807\n",
      "question: 558+008  pred: 0566 \u001b[92m☑\u001b[0m target: 0566\n",
      "--------------------------------------------------\n",
      "loss: 6.48885861664894e-06\n",
      "accuracy: 1.0\n",
      "question: 805+077  pred: 0882 \u001b[92m☑\u001b[0m target: 0882\n",
      "question: 751-047  pred: 0704 \u001b[92m☑\u001b[0m target: 0704\n",
      "question: 484-051  pred: 0433 \u001b[92m☑\u001b[0m target: 0433\n",
      "question: 120-006  pred: 0114 \u001b[92m☑\u001b[0m target: 0114\n",
      "question: 804-009  pred: 0795 \u001b[92m☑\u001b[0m target: 0795\n",
      "question: 368-090  pred: 0278 \u001b[92m☑\u001b[0m target: 0278\n",
      "question: 818-488  pred: 0330 \u001b[92m☑\u001b[0m target: 0330\n",
      "question: 553+225  pred: 0778 \u001b[92m☑\u001b[0m target: 0778\n",
      "question: 695-088  pred: 0607 \u001b[92m☑\u001b[0m target: 0607\n",
      "question: 627+045  pred: 0672 \u001b[92m☑\u001b[0m target: 0672\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 15 ******************************\n",
      "loss: 6.755856247764314e-06\n",
      "accuracy: 1.0\n",
      "question: 766-617  pred: 0149 \u001b[92m☑\u001b[0m target: 0149\n",
      "question: 223-001  pred: 0222 \u001b[92m☑\u001b[0m target: 0222\n",
      "question: 481-247  pred: 0234 \u001b[92m☑\u001b[0m target: 0234\n",
      "question: 654-140  pred: 0514 \u001b[92m☑\u001b[0m target: 0514\n",
      "question: 016+012  pred: 0028 \u001b[92m☑\u001b[0m target: 0028\n",
      "question: 101-082  pred: 0019 \u001b[92m☑\u001b[0m target: 0019\n",
      "question: 660+067  pred: 0727 \u001b[92m☑\u001b[0m target: 0727\n",
      "question: 064-016  pred: 0048 \u001b[92m☑\u001b[0m target: 0048\n",
      "question: 090+062  pred: 0152 \u001b[92m☑\u001b[0m target: 0152\n",
      "question: 986+070  pred: 1056 \u001b[92m☑\u001b[0m target: 1056\n",
      "--------------------------------------------------\n",
      "loss: 5.234594482317334e-06\n",
      "accuracy: 1.0\n",
      "question: 805-006  pred: 0799 \u001b[92m☑\u001b[0m target: 0799\n",
      "question: 886+007  pred: 0893 \u001b[92m☑\u001b[0m target: 0893\n",
      "question: 077-062  pred: 0015 \u001b[92m☑\u001b[0m target: 0015\n",
      "question: 822+049  pred: 0871 \u001b[92m☑\u001b[0m target: 0871\n",
      "question: 069+032  pred: 0101 \u001b[92m☑\u001b[0m target: 0101\n",
      "question: 205-006  pred: 0199 \u001b[92m☑\u001b[0m target: 0199\n",
      "question: 982+028  pred: 1010 \u001b[92m☑\u001b[0m target: 1010\n",
      "question: 739-049  pred: 0690 \u001b[92m☑\u001b[0m target: 0690\n",
      "question: 388+036  pred: 0424 \u001b[92m☑\u001b[0m target: 0424\n",
      "question: 712-069  pred: 0643 \u001b[92m☑\u001b[0m target: 0643\n",
      "--------------------------------------------------\n",
      "loss: 5.3523126553045586e-06\n",
      "accuracy: 1.0\n",
      "question: 218-098  pred: 0120 \u001b[92m☑\u001b[0m target: 0120\n",
      "question: 725-599  pred: 0126 \u001b[92m☑\u001b[0m target: 0126\n",
      "question: 709-000  pred: 0709 \u001b[92m☑\u001b[0m target: 0709\n",
      "question: 794-168  pred: 0626 \u001b[92m☑\u001b[0m target: 0626\n",
      "question: 763+169  pred: 0932 \u001b[92m☑\u001b[0m target: 0932\n",
      "question: 518+236  pred: 0754 \u001b[92m☑\u001b[0m target: 0754\n",
      "question: 292+068  pred: 0360 \u001b[92m☑\u001b[0m target: 0360\n",
      "question: 742-002  pred: 0740 \u001b[92m☑\u001b[0m target: 0740\n",
      "question: 022+014  pred: 0036 \u001b[92m☑\u001b[0m target: 0036\n",
      "question: 399+005  pred: 0404 \u001b[92m☑\u001b[0m target: 0404\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 16 ******************************\n",
      "loss: 1.9698940377566032e-05\n",
      "accuracy: 1.0\n",
      "question: 055+003  pred: 0058 \u001b[92m☑\u001b[0m target: 0058\n",
      "question: 945+653  pred: 1598 \u001b[92m☑\u001b[0m target: 1598\n",
      "question: 774-499  pred: 0275 \u001b[92m☑\u001b[0m target: 0275\n",
      "question: 680-215  pred: 0465 \u001b[92m☑\u001b[0m target: 0465\n",
      "question: 565+063  pred: 0628 \u001b[92m☑\u001b[0m target: 0628\n",
      "question: 481+173  pred: 0654 \u001b[92m☑\u001b[0m target: 0654\n",
      "question: 675+043  pred: 0718 \u001b[92m☑\u001b[0m target: 0718\n",
      "question: 872-009  pred: 0863 \u001b[92m☑\u001b[0m target: 0863\n",
      "question: 053-008  pred: 0045 \u001b[92m☑\u001b[0m target: 0045\n",
      "question: 677+411  pred: 1088 \u001b[92m☑\u001b[0m target: 1088\n",
      "--------------------------------------------------\n",
      "loss: 1.946830343513284e-05\n",
      "accuracy: 1.0\n",
      "question: 787-063  pred: 0724 \u001b[92m☑\u001b[0m target: 0724\n",
      "question: 231+009  pred: 0240 \u001b[92m☑\u001b[0m target: 0240\n",
      "question: 147+057  pred: 0204 \u001b[92m☑\u001b[0m target: 0204\n",
      "question: 105-007  pred: 0098 \u001b[92m☑\u001b[0m target: 0098\n",
      "question: 599-266  pred: 0333 \u001b[92m☑\u001b[0m target: 0333\n",
      "question: 102+004  pred: 0106 \u001b[92m☑\u001b[0m target: 0106\n",
      "question: 701-054  pred: 0647 \u001b[92m☑\u001b[0m target: 0647\n",
      "question: 206-083  pred: 0123 \u001b[92m☑\u001b[0m target: 0123\n",
      "question: 559+030  pred: 0589 \u001b[92m☑\u001b[0m target: 0589\n",
      "question: 108+026  pred: 0134 \u001b[92m☑\u001b[0m target: 0134\n",
      "--------------------------------------------------\n",
      "loss: 0.00362641760148108\n",
      "accuracy: 0.8828125\n",
      "question: 715+012  pred: 0727 \u001b[92m☑\u001b[0m target: 0727\n",
      "question: 926+537  pred: 1463 \u001b[92m☑\u001b[0m target: 1463\n",
      "question: 777-737  pred: 0040 \u001b[92m☑\u001b[0m target: 0040\n",
      "question: 669+157  pred: 0826 \u001b[92m☑\u001b[0m target: 0826\n",
      "question: 945-014  pred: 0931 \u001b[92m☑\u001b[0m target: 0931\n",
      "question: 530-026  pred: 0504 \u001b[92m☑\u001b[0m target: 0504\n",
      "question: 768+007  pred: 0775 \u001b[92m☑\u001b[0m target: 0775\n",
      "question: 887-528  pred: 0359 \u001b[92m☑\u001b[0m target: 0359\n",
      "question: 174+033  pred: 0207 \u001b[92m☑\u001b[0m target: 0207\n",
      "question: 278+061  pred: 0339 \u001b[92m☑\u001b[0m target: 0339\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 17 ******************************\n",
      "loss: 0.002362071769312024\n",
      "accuracy: 0.9296875\n",
      "question: 054-023  pred: 0031 \u001b[92m☑\u001b[0m target: 0031\n",
      "question: 120+015  pred: 0135 \u001b[92m☑\u001b[0m target: 0135\n",
      "question: 080-079  pred: 0001 \u001b[92m☑\u001b[0m target: 0001\n",
      "question: 518-012  pred: 0506 \u001b[92m☑\u001b[0m target: 0506\n",
      "question: 800+644  pred: 1544 \u001b[91m☒\u001b[0m target: 1444\n",
      "question: 871+066  pred: 0947 \u001b[91m☒\u001b[0m target: 0937\n",
      "question: 813+004  pred: 0817 \u001b[92m☑\u001b[0m target: 0817\n",
      "question: 387+094  pred: 0481 \u001b[92m☑\u001b[0m target: 0481\n",
      "question: 087+064  pred: 0151 \u001b[92m☑\u001b[0m target: 0151\n",
      "question: 097-001  pred: 0096 \u001b[92m☑\u001b[0m target: 0096\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0011491944314911962\n",
      "accuracy: 0.96875\n",
      "question: 575-325  pred: 0250 \u001b[92m☑\u001b[0m target: 0250\n",
      "question: 084+030  pred: 0114 \u001b[92m☑\u001b[0m target: 0114\n",
      "question: 152-008  pred: 0144 \u001b[92m☑\u001b[0m target: 0144\n",
      "question: 124+071  pred: 0195 \u001b[92m☑\u001b[0m target: 0195\n",
      "question: 772+001  pred: 0773 \u001b[92m☑\u001b[0m target: 0773\n",
      "question: 729-386  pred: 0343 \u001b[92m☑\u001b[0m target: 0343\n",
      "question: 309-082  pred: 0227 \u001b[92m☑\u001b[0m target: 0227\n",
      "question: 882+031  pred: 0913 \u001b[92m☑\u001b[0m target: 0913\n",
      "question: 082-027  pred: 0055 \u001b[92m☑\u001b[0m target: 0055\n",
      "question: 024-006  pred: 0018 \u001b[92m☑\u001b[0m target: 0018\n",
      "--------------------------------------------------\n",
      "loss: 0.0002307343966094777\n",
      "accuracy: 0.9921875\n",
      "question: 532+119  pred: 0651 \u001b[92m☑\u001b[0m target: 0651\n",
      "question: 770-022  pred: 0748 \u001b[92m☑\u001b[0m target: 0748\n",
      "question: 921+324  pred: 1245 \u001b[92m☑\u001b[0m target: 1245\n",
      "question: 529-275  pred: 0254 \u001b[92m☑\u001b[0m target: 0254\n",
      "question: 849-522  pred: 0327 \u001b[92m☑\u001b[0m target: 0327\n",
      "question: 184-143  pred: 0041 \u001b[92m☑\u001b[0m target: 0041\n",
      "question: 198-053  pred: 0145 \u001b[92m☑\u001b[0m target: 0145\n",
      "question: 460-388  pred: 0072 \u001b[92m☑\u001b[0m target: 0072\n",
      "question: 955-009  pred: 0946 \u001b[92m☑\u001b[0m target: 0946\n",
      "question: 859-101  pred: 0758 \u001b[92m☑\u001b[0m target: 0758\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 18 ******************************\n",
      "loss: 6.419889541575685e-05\n",
      "accuracy: 1.0\n",
      "question: 241-077  pred: 0164 \u001b[92m☑\u001b[0m target: 0164\n",
      "question: 602-092  pred: 0510 \u001b[92m☑\u001b[0m target: 0510\n",
      "question: 770+075  pred: 0845 \u001b[92m☑\u001b[0m target: 0845\n",
      "question: 251+054  pred: 0305 \u001b[92m☑\u001b[0m target: 0305\n",
      "question: 410-000  pred: 0410 \u001b[92m☑\u001b[0m target: 0410\n",
      "question: 589-040  pred: 0549 \u001b[92m☑\u001b[0m target: 0549\n",
      "question: 427-298  pred: 0129 \u001b[92m☑\u001b[0m target: 0129\n",
      "question: 608-573  pred: 0035 \u001b[92m☑\u001b[0m target: 0035\n",
      "question: 634+075  pred: 0709 \u001b[92m☑\u001b[0m target: 0709\n",
      "question: 963-008  pred: 0955 \u001b[92m☑\u001b[0m target: 0955\n",
      "--------------------------------------------------\n",
      "loss: 3.331403786432929e-05\n",
      "accuracy: 1.0\n",
      "question: 460+425  pred: 0885 \u001b[92m☑\u001b[0m target: 0885\n",
      "question: 700+047  pred: 0747 \u001b[92m☑\u001b[0m target: 0747\n",
      "question: 464+187  pred: 0651 \u001b[92m☑\u001b[0m target: 0651\n",
      "question: 488-076  pred: 0412 \u001b[92m☑\u001b[0m target: 0412\n",
      "question: 613-046  pred: 0567 \u001b[92m☑\u001b[0m target: 0567\n",
      "question: 774-388  pred: 0386 \u001b[92m☑\u001b[0m target: 0386\n",
      "question: 759-097  pred: 0662 \u001b[92m☑\u001b[0m target: 0662\n",
      "question: 083+039  pred: 0122 \u001b[92m☑\u001b[0m target: 0122\n",
      "question: 219+054  pred: 0273 \u001b[92m☑\u001b[0m target: 0273\n",
      "question: 790+003  pred: 0793 \u001b[92m☑\u001b[0m target: 0793\n",
      "--------------------------------------------------\n",
      "loss: 0.00016125843103509396\n",
      "accuracy: 1.0\n",
      "question: 077-073  pred: 0004 \u001b[92m☑\u001b[0m target: 0004\n",
      "question: 482+036  pred: 0518 \u001b[92m☑\u001b[0m target: 0518\n",
      "question: 125-124  pred: 0001 \u001b[92m☑\u001b[0m target: 0001\n",
      "question: 759-543  pred: 0216 \u001b[92m☑\u001b[0m target: 0216\n",
      "question: 744-063  pred: 0681 \u001b[92m☑\u001b[0m target: 0681\n",
      "question: 086+085  pred: 0171 \u001b[92m☑\u001b[0m target: 0171\n",
      "question: 318-027  pred: 0291 \u001b[92m☑\u001b[0m target: 0291\n",
      "question: 586-027  pred: 0559 \u001b[92m☑\u001b[0m target: 0559\n",
      "question: 844+014  pred: 0858 \u001b[92m☑\u001b[0m target: 0858\n",
      "question: 907-002  pred: 0905 \u001b[92m☑\u001b[0m target: 0905\n",
      "--------------------------------------------------\n",
      "****************************** epoch: 19 ******************************\n",
      "loss: 2.6721880203695036e-05\n",
      "accuracy: 1.0\n",
      "question: 700-241  pred: 0459 \u001b[92m☑\u001b[0m target: 0459\n",
      "question: 627+017  pred: 0644 \u001b[92m☑\u001b[0m target: 0644\n",
      "question: 479-097  pred: 0382 \u001b[92m☑\u001b[0m target: 0382\n",
      "question: 666+048  pred: 0714 \u001b[92m☑\u001b[0m target: 0714\n",
      "question: 949+893  pred: 1842 \u001b[92m☑\u001b[0m target: 1842\n",
      "question: 160-038  pred: 0122 \u001b[92m☑\u001b[0m target: 0122\n",
      "question: 929-084  pred: 0845 \u001b[92m☑\u001b[0m target: 0845\n",
      "question: 862+092  pred: 0954 \u001b[92m☑\u001b[0m target: 0954\n",
      "question: 044-006  pred: 0038 \u001b[92m☑\u001b[0m target: 0038\n",
      "question: 766+028  pred: 0794 \u001b[92m☑\u001b[0m target: 0794\n",
      "--------------------------------------------------\n",
      "loss: 0.0006907186470925808\n",
      "accuracy: 0.984375\n",
      "question: 695+036  pred: 0631 \u001b[91m☒\u001b[0m target: 0731\n",
      "question: 540-110  pred: 0430 \u001b[92m☑\u001b[0m target: 0430\n",
      "question: 872+741  pred: 1613 \u001b[92m☑\u001b[0m target: 1613\n",
      "question: 598-409  pred: 0189 \u001b[92m☑\u001b[0m target: 0189\n",
      "question: 711+005  pred: 0716 \u001b[92m☑\u001b[0m target: 0716\n",
      "question: 523+008  pred: 0531 \u001b[92m☑\u001b[0m target: 0531\n",
      "question: 020-010  pred: 0010 \u001b[92m☑\u001b[0m target: 0010\n",
      "question: 851-732  pred: 0119 \u001b[92m☑\u001b[0m target: 0119\n",
      "question: 114+071  pred: 0185 \u001b[92m☑\u001b[0m target: 0185\n",
      "question: 935+065  pred: 1000 \u001b[92m☑\u001b[0m target: 1000\n",
      "--------------------------------------------------\n",
      "loss: 0.00041947781573981047\n",
      "accuracy: 0.984375\n",
      "question: 321-001  pred: 0320 \u001b[92m☑\u001b[0m target: 0320\n",
      "question: 718+080  pred: 0798 \u001b[92m☑\u001b[0m target: 0798\n",
      "question: 079+031  pred: 0110 \u001b[92m☑\u001b[0m target: 0110\n",
      "question: 448+016  pred: 0464 \u001b[92m☑\u001b[0m target: 0464\n",
      "question: 383-001  pred: 0382 \u001b[92m☑\u001b[0m target: 0382\n",
      "question: 524+059  pred: 0583 \u001b[92m☑\u001b[0m target: 0583\n",
      "question: 793+243  pred: 1036 \u001b[92m☑\u001b[0m target: 1036\n",
      "question: 277-228  pred: 0049 \u001b[92m☑\u001b[0m target: 0049\n",
      "question: 625-033  pred: 0592 \u001b[92m☑\u001b[0m target: 0592\n",
      "question: 560-047  pred: 0513 \u001b[92m☑\u001b[0m target: 0513\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"USE_CUDA:\", USE_CUDA)    \n",
    "    \n",
    "rnn = RNN()\n",
    "if(USE_CUDA):\n",
    "    rnn.cuda()\n",
    "print(rnn)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "h_state = None      # for initial hidden state\n",
    "for epoch in range(EPOCH):\n",
    "    print('*'*30, \"epoch:\",epoch, '*'*30)\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):  # 每一步 loader 释放一小批数据用来学习\n",
    "        \n",
    "        if(USE_CUDA):\n",
    "            prediction = rnn(batch_x.cuda())   # rnn output\n",
    "            loss = loss_func(prediction.cuda(), batch_y.cuda())\n",
    "        else:\n",
    "            prediction = rnn(batch_x)\n",
    "            loss = loss_func(prediction, batch_y)\n",
    "            \n",
    "        optimizer.zero_grad()                   # clear gradients for this training step\n",
    "        loss.backward(retain_graph=True)        # backpropagation, compute gradients\n",
    "        optimizer.step()                        # apply gradients        \n",
    "        \n",
    "        if(step%100 == 0):\n",
    "            print(\"loss:\",loss.data.item())\n",
    "            \n",
    "            show_pred = np.array([ctable.decode(b.data.cpu().numpy()) for b in prediction])\n",
    "            show_y = np.array([ctable.decode(b.data.cpu().numpy()) for b in batch_y])\n",
    "\n",
    "            acc = sum(show_pred == show_y)/BATCH_SIZE\n",
    "            print(\"accuracy:\", acc)\n",
    "\n",
    "            for i in range(10):\n",
    "                question = ctable.decode(batch_x[i].data.numpy())\n",
    "                print(\"question:\", question ,end='  ')\n",
    "                print(\"pred:\", show_pred[i] ,end=' ')\n",
    "                if show_pred[i] == show_y[i]:\n",
    "                    print(colors.ok + '☑' + colors.close, end=' ')\n",
    "                else:\n",
    "                    print(colors.fail + '☒' + colors.close, end=' ')\n",
    "                print(\"target:\", show_y[i])    \n",
    "\n",
    "            print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save and restore model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): LSTM(12, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (out): Linear(in_features=256, out_features=12, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model_name = 'subtracter_torch_rnn.pkl'\n",
    "model_params_name = 'subtracter_torch_rnn_params.pkl'\n",
    "\n",
    "torch.save(rnn, model_name)  # 保存整个网络\n",
    "torch.save(rnn.state_dict(), model_params_name)   # 只保存网络中的参数 (速度快, 占内存少)\n",
    "\n",
    "def restore_net():\n",
    "    # restore entire net1 to net2\n",
    "    rnn = torch.load(model_name)\n",
    "    return rnn\n",
    "    \n",
    "def restore_params():\n",
    "    # 新建 rnn\n",
    "    rnn = RNN()  #上面已定義的RNN\n",
    "    rnn.load_state_dict(torch.load(model_params_name)) # 将保存的参数复制到 rnns\n",
    "    return rnn\n",
    "    \n",
    "rnn = restore_net()\n",
    "if(USE_CUDA):\n",
    "    rnn.cuda()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size: torch.Size([40000, 7, 12])\n"
     ]
    }
   ],
   "source": [
    "print(\"test size:\", test_x_torch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.99405\n",
      "39762 / 40000\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    total_correct = 0\n",
    "    acc = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(test_loader):  # 每一步 loader 释放一小批数据用来学习\n",
    "        if(USE_CUDA):\n",
    "            prediction = rnn(batch_x.cuda())   # rnn output\n",
    "        else:\n",
    "            prediction = rnn(batch_x)\n",
    "\n",
    "        if(USE_CUDA):\n",
    "            pred = np.array([ctable.decode(b.data.cpu().numpy()) for b in prediction])\n",
    "            y = np.array([ctable.decode(b.data.cpu().numpy()) for b in batch_y])\n",
    "        else:\n",
    "            pred = np.array([ctable.decode(b.data.numpy()) for b in prediction])\n",
    "            y = np.array([ctable.decode(b.data.numpy()) for b in batch_y])\n",
    "        \n",
    "        total_correct = total_correct + sum(pred == y)\n",
    "\n",
    "    acc = total_correct / test_x_torch.size(0)\n",
    "    print(\"accuracy:\", acc)\n",
    "    print(total_correct, '/', test_x_torch.size(0) )\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
